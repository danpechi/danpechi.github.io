---
layout: page
title: About
permalink: /about/
---

Hi! I'm Dan; I'm currently a Data Scientist at [Comlinkdata](https://comlinkdata.com/), a telecom market research firm. I graduated from Tufts in 2019 with majors in Computational Linguistics, Cognitive Brain Science, and a minor in Russian. Previously, I built machine learning products for clients as a Data Science Consultant at Oliver Wyman Digital, and for the government as a Part-Time Software Engineer at Charles River Analytics.

When not modeling the telecom world, I read (predominantly NLP) papers, and write about them here; my previous blog from my undergrad years is [here](https://dandeeplearningblog.wordpress.com/)

Here's some cool stuff I did not so long ago:

[Meta-Turing Test Fall 2017](https://joehowarth.github.io/meta_turing/) 

[Meta-Turing Test Read Me](https://docs.google.com/document/d/1I7kJ_GkVkm7A2TzrnS6BON5A1o1dZPvD0GrnZkoI1_w/edit?usp=sharing)

This was my final I did with Joe Howarth for my NLP class in undergrad. My work is analagous to, independently designed from, and a far worse version of [Li et al. 2017](https://arxiv.org/pdf/1701.06547.pdf). We weren't able to link up feedback from the discriminator into the language model, so we don't quite get to a generative adversarial network (GAN).
We had the goal of also looping human feedback into the model to act as a sort of human-in-the-loop GAN, but this was a pain in the butt. In effect, you get to see if you are better than a discriminator AI at determining if the language model is human or not.
If you perform worse than the AI, the singularity begins and humanity is doomed.

[My Thesis](https://docs.google.com/document/d/1-RJDZJiX-w2XayW31BXrsAWKvvcF37CJs6FIE7uGpzM/edit?usp=sharing)

I got to work from 2017 to 2019 at a defense research contractor using news events to predict cyber attacks. This started out with an sklearn model being trained on keywords from news articles, to a BERT model using full news snippets.
Related submissions to relevant NLP and Cybersecurity/NLP conferences were unfruitful, turns out you can't reproduce data, and the resulting experiments, given to you by the government. I also got to explore and parse the Russian dark web to identify relevant hacker forums and marketplaces.

[Cognitive Science Spring 2018 Final](https://docs.google.com/document/d/1oSZ9GBqXkuqUYJAGGtMHYmW2rb66PWvJQVUNHcXPCJM/edit?usp=sharing)

A Review of Memory, Translation, and Bilingualism. I touch on the cognitive science behind these ideas, as well as their relevance to machine translation algorithms.

[Reinforcement Learning Fall 2018 Final](https://www.eecs.tufts.edu/~jsinapov/teaching/comp150_RL/proposals/pechi_shih_sun_proposal.pdf)

Along with my collaborators Jeremy Shih and Rui (Shari) Sun, I looked at inverse reinforcement learning (IRL) applications to gaming. Nothing crazy here, more of a lit review and trial run of code from said lit review.
I think IRL is very cool, I posted about it on my old blog [here](https://dandeeplearningblog.wordpress.com/2019/01/03/12-23-18-inverse-reinforcement-learning-and-theory-of-mind/) 

Relevant Coursework:

Language: Intermediate Russian (AY 2015-2016), Advanced Russian (AY 2016-2017), Beginner Russian Teaching Assistant (AY 2017-2019)

Coding: Intro to Computer Science (Fall 2016), Data Structures (Spring 2017), Natural Language Processing (Fall 2017), Reinforcement Learning (Fall 2018), Working with Corpora Teaching Assistant (Fall 2018), Introduction to Machine Learning (Spring 2019) 

Math: Discrete Math (Fall 2017), Probabilistic Systems Analysis (Spring 2018), Information Theory (Spring 2018), Mathematical Aspects of Data Analysis (Spring 2019)

Linguistics: Philosophy of Language (Fall 2016), Intro to Linguistics (Fall 2017), Second Language Acquisition (Fall 2018), Bilingual Studies (Spring 2019)


